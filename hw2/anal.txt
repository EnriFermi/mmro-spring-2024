Анализ:
1. Решения с большим размером data и размер батча 
имеют более гладкую кривую потерь и точности 
(64_12288_300, 64_12288_100, 16_3072_300, 16_3072_100).
Причем не наблюдается увеличения скорости сходимости
при более частом обновлении градиента по data
(64_12288_300, 16_3072_300)
2. Слишком редкие обновления градиента по data приводят
к нестабильности в обучении (2_512_500, 2_256_500,
2_1024_500)
3. Если сравнивать точность и loss на валидации у baseline
модели и данных моделей, то модели с размерами батча
16-64 достигают такого же качества, хотя к примеру для
16_3072_300 на 1 итерации происходит 26.24
вызовов оракула в то, время как для baseline модели данное
значение равно 1024
4. На графиках дисперсий градиентов четко виден паттерн,
когда дисперсия возрастает, а потом происходит резкое 
падение. Причем период паттерна равен частоте
обновления градиента data. Но при редком обновлении
градиентов data эффект снижения дисперсии практически
незаметен из-за большой осциляции дисперсии от итерации
к итерации. Если сравнивать дисперсию baseline c высокой
дисперсией то заметны сильные улучшения, так при
частом обновлении градиента дисперсия, во первых, стабильно
убывает (чего не было в baseline высокодисперсной модели),
во-вторых отсутсвуют сильные выбросы дисперсии, 
как в случае baseline. Если сравнивать низкодисперсную
модель с моделями на батчах 16-64 и частым обновлением,
то достигается очень схожие показатели при значительно
более низких мощностных затратах.

5. Анализ времени дает слабую зависимость 
времени от параметров обучения, то есть больший размер
батча приводит к более долгому обучению. Но присутствуют
выбросы (2_256_300), которые можно списать на распределение
ресурсов при конкретном запуске.
Итог, SVRG действительно при схожих параметрах обучения
позволяет существенно уменьшить дисперсию модели
и повысить стабильность обучения и качество 
предсказаний модели.

